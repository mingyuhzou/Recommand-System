{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "48f4acb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DirEntry',\n",
       " 'F_OK',\n",
       " 'GenericAlias',\n",
       " 'Mapping',\n",
       " 'MutableMapping',\n",
       " 'O_APPEND',\n",
       " 'O_BINARY',\n",
       " 'O_CREAT',\n",
       " 'O_EXCL',\n",
       " 'O_NOINHERIT',\n",
       " 'O_RANDOM',\n",
       " 'O_RDONLY',\n",
       " 'O_RDWR',\n",
       " 'O_SEQUENTIAL',\n",
       " 'O_SHORT_LIVED',\n",
       " 'O_TEMPORARY',\n",
       " 'O_TEXT',\n",
       " 'O_TRUNC',\n",
       " 'O_WRONLY',\n",
       " 'P_DETACH',\n",
       " 'P_NOWAIT',\n",
       " 'P_NOWAITO',\n",
       " 'P_OVERLAY',\n",
       " 'P_WAIT',\n",
       " 'PathLike',\n",
       " 'R_OK',\n",
       " 'SEEK_CUR',\n",
       " 'SEEK_END',\n",
       " 'SEEK_SET',\n",
       " 'TMP_MAX',\n",
       " 'W_OK',\n",
       " 'X_OK',\n",
       " '_AddedDllDirectory',\n",
       " '_Environ',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_check_methods',\n",
       " '_execvpe',\n",
       " '_exists',\n",
       " '_exit',\n",
       " '_fspath',\n",
       " '_get_exports_list',\n",
       " '_walk',\n",
       " '_wrap_close',\n",
       " 'abc',\n",
       " 'abort',\n",
       " 'access',\n",
       " 'add_dll_directory',\n",
       " 'altsep',\n",
       " 'chdir',\n",
       " 'chmod',\n",
       " 'close',\n",
       " 'closerange',\n",
       " 'cpu_count',\n",
       " 'curdir',\n",
       " 'defpath',\n",
       " 'device_encoding',\n",
       " 'devnull',\n",
       " 'dup',\n",
       " 'dup2',\n",
       " 'environ',\n",
       " 'error',\n",
       " 'execl',\n",
       " 'execle',\n",
       " 'execlp',\n",
       " 'execlpe',\n",
       " 'execv',\n",
       " 'execve',\n",
       " 'execvp',\n",
       " 'execvpe',\n",
       " 'extsep',\n",
       " 'fdopen',\n",
       " 'fsdecode',\n",
       " 'fsencode',\n",
       " 'fspath',\n",
       " 'fstat',\n",
       " 'fsync',\n",
       " 'ftruncate',\n",
       " 'get_exec_path',\n",
       " 'get_handle_inheritable',\n",
       " 'get_inheritable',\n",
       " 'get_terminal_size',\n",
       " 'getcwd',\n",
       " 'getcwdb',\n",
       " 'getenv',\n",
       " 'getlogin',\n",
       " 'getpid',\n",
       " 'getppid',\n",
       " 'isatty',\n",
       " 'kill',\n",
       " 'linesep',\n",
       " 'link',\n",
       " 'listdir',\n",
       " 'lseek',\n",
       " 'lstat',\n",
       " 'makedirs',\n",
       " 'mkdir',\n",
       " 'name',\n",
       " 'open',\n",
       " 'pardir',\n",
       " 'path',\n",
       " 'pathsep',\n",
       " 'pipe',\n",
       " 'popen',\n",
       " 'putenv',\n",
       " 'read',\n",
       " 'readlink',\n",
       " 'remove',\n",
       " 'removedirs',\n",
       " 'rename',\n",
       " 'renames',\n",
       " 'replace',\n",
       " 'rmdir',\n",
       " 'scandir',\n",
       " 'sep',\n",
       " 'set_handle_inheritable',\n",
       " 'set_inheritable',\n",
       " 'spawnl',\n",
       " 'spawnle',\n",
       " 'spawnv',\n",
       " 'spawnve',\n",
       " 'st',\n",
       " 'startfile',\n",
       " 'stat',\n",
       " 'stat_result',\n",
       " 'statvfs_result',\n",
       " 'strerror',\n",
       " 'supports_bytes_environ',\n",
       " 'supports_dir_fd',\n",
       " 'supports_effective_ids',\n",
       " 'supports_fd',\n",
       " 'supports_follow_symlinks',\n",
       " 'symlink',\n",
       " 'sys',\n",
       " 'system',\n",
       " 'terminal_size',\n",
       " 'times',\n",
       " 'times_result',\n",
       " 'truncate',\n",
       " 'umask',\n",
       " 'uname_result',\n",
       " 'unlink',\n",
       " 'unsetenv',\n",
       " 'urandom',\n",
       " 'utime',\n",
       " 'waitpid',\n",
       " 'waitstatus_to_exitcode',\n",
       " 'walk',\n",
       " 'write']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a7032f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "class Mydata(Dataset):\n",
    "    def __init__(self,root_path,label_path):\n",
    "        self.root_path=root_path\n",
    "        self.label_path=label_path\n",
    "        self.img_path=os.path.join(self.root_path,self.label_path)\n",
    "        self.img_list=os.listdir(self.img_path)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name=self.img_list[idx]\n",
    "        img_item_path=os.path.join(self.img_path,img_name)\n",
    "        img=Image.open(img_item_path)\n",
    "        label=self.label_path\n",
    "        return img,label \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5dc1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path=\"E:\\\\DL\\\\hymenoptera_data\\\\hymenoptera_data\\\\train\"\n",
    "ants=Mydata(root_path,'ants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "182d6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,_=ants[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02a4a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bees=Mydata(root_path,'bees')\n",
    "bee_img,_=bees[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3cd38035",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalData=ants+bees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0405e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "img_tensor=ToTensor()\n",
    "img=img_tensor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71fd9e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8cf882de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "trans_norm=transforms.Normalize([0.5,0.5,0.5],[0.5,.5,0.5])\n",
    "norm_img=trans_norm(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93c57115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer=SummaryWriter('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "40ad81c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    writer.add_scalar('y=4x',4*i,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "011040e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_image('ants',img,global_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ea36896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_image('ants',norm_img,global_step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dedc38f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 375)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orginal_img,_=ants[11]\n",
    "orginal_img.show()\n",
    "orginal_img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resizer=transforms.Resize((400,300))\n",
    "resize_img=Resizer(orginal_img)\n",
    "# resize_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8dc76f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_compose=transforms.Compose([transforms.Resize((400,300)),transforms.ToTensor(),transforms.Normalize([0.5,.5,.5],[.5,.5,.5])])\n",
    "\n",
    "img_compose=trans_compose(bees[101][0])\n",
    "\n",
    "writer.add_image('compose image',img_compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b48d7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 200, 200])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crop=transforms.RandomCrop([200,200],pad_if_needed=True,padding_mode='edge')\n",
    "crop_img=Crop(orginal_img)\n",
    "\n",
    "tmp=img_tensor(crop_img)\n",
    "tmp=trans_norm(tmp)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "85c862c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropCompose=transforms.Compose([Crop,img_tensor])\n",
    "i=0\n",
    "for k,_ in ants:\n",
    "    writer.add_image('crop',cropCompose(k),i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "918c076e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 200, 200])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp=trans_norm(cropCompose(ants[11][0]))\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d1304ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compose=transforms.Compose([cropCompose,trans_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5b7e816c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4196,  0.3882,  0.4039,  ...,  0.2235,  0.2078,  0.2314],\n",
       "         [ 0.3961,  0.3961,  0.3882,  ...,  0.2157,  0.2000,  0.2235],\n",
       "         [ 0.3804,  0.3882,  0.3804,  ...,  0.2157,  0.2078,  0.2549],\n",
       "         ...,\n",
       "         [-0.3725, -0.3804, -0.3961,  ..., -0.2314, -0.2471, -0.2157],\n",
       "         [-0.4510, -0.4275, -0.4353,  ..., -0.1922, -0.2157, -0.2000],\n",
       "         [-0.4510, -0.4980, -0.4745,  ..., -0.1608, -0.1765, -0.1686]],\n",
       "\n",
       "        [[ 0.4196,  0.3882,  0.4039,  ...,  0.1843,  0.1686,  0.2000],\n",
       "         [ 0.3882,  0.3961,  0.3882,  ...,  0.2000,  0.1686,  0.1922],\n",
       "         [ 0.3804,  0.3882,  0.3804,  ...,  0.2078,  0.2000,  0.2314],\n",
       "         ...,\n",
       "         [-0.4039, -0.4118, -0.4039,  ..., -0.2471, -0.2627, -0.2314],\n",
       "         [-0.5137, -0.4902, -0.4980,  ..., -0.2000, -0.2235, -0.2078],\n",
       "         [-0.5373, -0.5843, -0.5608,  ..., -0.1765, -0.1843, -0.1765]],\n",
       "\n",
       "        [[ 0.4196,  0.3882,  0.4039,  ...,  0.2314,  0.2000,  0.2078],\n",
       "         [ 0.3569,  0.3804,  0.4039,  ...,  0.2078,  0.1451,  0.1686],\n",
       "         [ 0.3647,  0.3882,  0.3961,  ...,  0.1922,  0.1686,  0.1922],\n",
       "         ...,\n",
       "         [-0.6314, -0.6392, -0.6392,  ..., -0.4275, -0.4431, -0.4275],\n",
       "         [-0.6784, -0.6706, -0.6784,  ..., -0.3569, -0.3804, -0.3647],\n",
       "         [-0.6784, -0.7255, -0.7176,  ..., -0.3412, -0.3412, -0.3333]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compose(ants[11][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "02ec1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropCompose1=transforms.Compose([Crop,img_tensor])\n",
    "cropCompose2=transforms.Compose([Crop,img_tensor,trans_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fb9c890f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7255, -0.9922, -0.6863,  ...,  0.4510,  0.4431,  0.4431],\n",
       "         [-0.7647, -0.9373, -1.0000,  ...,  0.4510,  0.4431,  0.4431],\n",
       "         [-0.7647, -0.8275, -1.0000,  ...,  0.4353,  0.4353,  0.4353],\n",
       "         ...,\n",
       "         [-0.1216, -0.0196, -0.0510,  ...,  0.0196,  0.0353,  0.0353],\n",
       "         [ 0.0118,  0.0118, -0.0588,  ...,  0.0588,  0.0745,  0.0431],\n",
       "         [-0.0039,  0.0667, -0.0353,  ...,  0.0745,  0.0510,  0.0510]],\n",
       "\n",
       "        [[-0.7725, -1.0000, -0.6863,  ...,  0.4510,  0.4431,  0.4431],\n",
       "         [-0.8118, -0.9765, -0.9843,  ...,  0.4510,  0.4431,  0.4431],\n",
       "         [-0.8353, -0.8745, -0.9843,  ...,  0.4510,  0.4431,  0.4431],\n",
       "         ...,\n",
       "         [-0.0510,  0.0431,  0.0118,  ...,  0.0039,  0.0039,  0.0039],\n",
       "         [ 0.0745,  0.0745, -0.0196,  ...,  0.0118,  0.0196,  0.0118],\n",
       "         [ 0.0353,  0.0980, -0.0039,  ...,  0.0039, -0.0275, -0.0275]],\n",
       "\n",
       "        [[-0.7412, -0.9843, -0.7020,  ...,  0.4510,  0.4431,  0.4588],\n",
       "         [-0.7804, -0.9451, -1.0000,  ...,  0.4510,  0.4588,  0.4588],\n",
       "         [-0.7961, -0.8431, -1.0000,  ...,  0.4431,  0.4588,  0.4588],\n",
       "         ...,\n",
       "         [-0.2157, -0.1216, -0.1059,  ..., -0.0902, -0.0824, -0.0824],\n",
       "         [-0.1137, -0.1059, -0.1765,  ..., -0.0824, -0.0588, -0.0745],\n",
       "         [-0.1216, -0.0353, -0.1216,  ..., -0.0667, -0.0980, -0.0980]]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropCompose1(ants[11][0])\n",
    "cropCompose2(ants[11][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26e817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 512)\n",
      "(500, 333)\n",
      "(500, 282)\n",
      "(500, 335)\n",
      "(500, 348)\n",
      "(500, 321)\n",
      "(500, 500)\n",
      "(500, 333)\n",
      "(500, 375)\n",
      "(500, 375)\n",
      "(500, 375)\n",
      "(500, 375)\n",
      "(441, 500)\n",
      "(500, 375)\n",
      "(500, 400)\n",
      "(500, 375)\n",
      "(500, 375)\n",
      "(500, 374)\n",
      "(343, 496)\n",
      "(500, 333)\n",
      "(500, 415)\n",
      "(500, 330)\n",
      "(500, 333)\n",
      "(479, 359)\n",
      "(500, 462)\n",
      "(500, 314)\n",
      "(500, 375)\n",
      "(500, 361)\n",
      "(500, 423)\n",
      "(500, 400)\n",
      "(500, 333)\n",
      "(500, 375)\n",
      "(500, 333)\n",
      "(500, 375)\n",
      "(500, 236)\n",
      "(500, 375)\n",
      "(500, 375)\n",
      "(500, 333)\n",
      "(500, 409)\n",
      "(500, 409)\n",
      "(500, 434)\n",
      "(500, 334)\n",
      "(500, 500)\n",
      "(500, 500)\n",
      "(500, 333)\n",
      "(500, 375)\n",
      "(500, 325)\n",
      "(500, 357)\n",
      "(357, 500)\n",
      "(500, 375)\n",
      "(500, 375)\n",
      "(375, 500)\n",
      "(500, 333)\n",
      "(500, 375)\n",
      "(500, 375)\n",
      "(500, 375)\n",
      "(399, 500)\n",
      "(488, 500)\n",
      "(500, 358)\n",
      "(500, 384)\n",
      "(500, 333)\n",
      "(500, 333)\n",
      "(500, 356)\n",
      "(500, 333)\n",
      "(500, 375)\n",
      "(500, 407)\n",
      "(500, 333)\n",
      "(500, 374)\n",
      "(500, 407)\n",
      "(500, 444)\n",
      "(500, 374)\n",
      "(500, 333)\n",
      "(333, 500)\n",
      "(333, 500)\n",
      "(500, 397)\n",
      "(500, 313)\n",
      "(500, 333)\n",
      "(500, 333)\n",
      "(500, 339)\n",
      "(500, 400)\n",
      "(500, 375)\n",
      "(500, 332)\n",
      "(500, 333)\n",
      "(417, 500)\n",
      "(500, 375)\n",
      "(500, 239)\n",
      "(500, 333)\n",
      "(500, 402)\n",
      "(500, 432)\n",
      "(500, 375)\n",
      "(500, 375)\n",
      "(500, 369)\n",
      "(500, 181)\n",
      "(500, 369)\n",
      "(500, 146)\n",
      "(500, 500)\n",
      "(375, 500)\n",
      "(500, 328)\n",
      "(500, 333)\n",
      "(500, 366)\n",
      "(500, 333)\n",
      "(375, 500)\n",
      "(500, 375)\n",
      "(415, 500)\n",
      "(360, 270)\n",
      "(500, 372)\n",
      "(375, 500)\n",
      "(500, 375)\n",
      "(500, 375)\n",
      "(500, 333)\n",
      "(500, 375)\n",
      "(343, 236)\n",
      "(199, 272)\n",
      "(200, 137)\n",
      "(193, 130)\n",
      "(320, 238)\n",
      "(300, 300)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 200, 200] doesn't match the broadcast shape [3, 200, 200]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,_ \u001b[38;5;129;01min\u001b[39;00m ants:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(k\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mcropCompose2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    927\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv_(std)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: output with shape [1, 200, 200] doesn't match the broadcast shape [3, 200, 200]"
     ]
    }
   ],
   "source": [
    "cropCompose2=transforms.Compose([Crop,img_tensor,trans_norm])\n",
    "cropCompose2(ants[11][0])\n",
    "for k,_ in ants:\n",
    "    cropCompose2(k)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96c118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
