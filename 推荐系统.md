#  概要

## 定义

推荐系统可以被定义为：**对于某个用户，在特定的场景下，针对海量的物品信息构建一个函数，函数预测用户对特定候选物品的喜好程度，再根据喜好程度对所有物品进行排序，生成推荐列表。**

推荐系统是一个应用属性很强的领域，需要使用各种各样的技术，**推荐模型只是推荐系统中的一个模块**，而非全部，为了让模型能够正常运行，发挥作用，推荐系统中需要以下众多模块协同工作：

1. 日志系统收集用户反馈，为推荐系统提供原始数据；
2. 大数据系统、流式计算系统从原始数据中提取信息，将它们加工成模型所需的形式
3. 在线学习系统及时更新模型
4. 监控系统让我们观察模型运行是否正常，并且能够自动报警
5. A/B实验系统评估模型效果，并且能够动态配置、改变推荐系统的运行方式

## 推荐系统指标

**北极星指标**

- 用户规模：**日活用户数DAU**——一天内多次使用只算一次日活，**月活用户数MAU**——一月内登录多次使用只算一次月活。
- 消费：**人均使用推荐的时长，人均阅读笔记的数量**
- 发布：**发布渗透率，人均发布量**

## 实验流程

![image-20251012142757838](./assets/image-20251012142757838.png)

离线实验：收集**历史数据**，在历史数据上做**训练，测试**。**算法没有部署到产品中，没有跟用户交互。**

小流量AB实验：把**算法部署到实际产品**中，用户实际跟算法做**交互**。用户分为**实验组和对照组，实验组用新策略，对照组用旧策略**，比较实验组是否**优于**对照组，如果优于则可以**加大流量最终推全。**



## 推荐系统的链路

<img src="./assets/image-20251027164433191.png" alt="image-20251027164433191" style="zoom:67%;" />

- 召回：用多条通道，取回几千篇笔记
- 粗排：用小规模神经网络给笔记打分，选出分数最高的几百篇
- 精排：用大规模神经网络，给几百篇笔记打分
- 重排：做多样性抽样，规则打散，插入广告和运营笔记



## 大数据架构



以视频推荐为例，为了让模型了解每个视频的受欢迎程度，需要计算每个视频的点击率CTR。计算CTR有3方面的难点

1. 为了保证统计结果的**有效性**，需要将统计窗口拉大，比例统计过去一周每个视频的CTR，**但是要回溯的历史越长，所涉及到的计算量就越大**。
2. 此外线上做预测的**时间非常紧张**，留给所有特征的准备时间不会超过10ms。
3. 由于Hadoop分布式文件系统(HDFS)**只支持批量数据读写**，因此**有许多即时的用户行为还没来得及组成用户日志或落盘到HDFS**，导致出现冷热数据的时间差，只能用到**过时的数据**。



为了应对大数据系统的复杂性，lambda架构应运而生

+ **将数据请求还结尾分别针对冷热数据的两个子请求**
+ **针对冷数据的请求，由离线曾批量完成计算，其结果由近线层缓存并提供快速查询**
+ **针对热数据的请求，由在线层基于流式算法进行处理**
+ **汇总冷热数据的结果，一般是将两个指标都喂给模型，使模型具备不同时间维度的视角**

![image-20251027171645476](./assets/image-20251027171645476.png)



离线层：**启动小时级的定时任务，每个小时都想浅灰塑一周的用户行为日志，统计数据。**

近线层：**将离线层计算后的结果导入键值型数据库，加速访问。**

在线层：**使用Flink，Storm等六十计算框架，对接用户的行为数据流，不等数据落地就直接进行分析，计算结果也缓存到键值数据库中。**



## A/B测试

AB测试考察**线上指标的影响，以及如何选取最优的参数**。

<img src="./assets/image-20251018155610124.png" alt="image-20251018155610124" style="zoom:50%;" />

随机分桶，将用户ID用**哈希函数**映射为一个整数，分为多个桶，用户ID落在那个桶中就按照桶的策略做推荐。

<img src="./assets/image-20251012144724666.png" alt="image-20251012144724666" style="zoom:50%;" />



一个公司中存在很多部门和团队，如果大家都需要做A/B测试，会导致**流量不够**，因此引入了**分层实验**。



分层实验：**同层互斥，不同层正交**。以召回层为例，两组召回层实验不能作用与一个用户身上，但是一个用户可以受到一个召回层和一个精排层的影响。

有的同类策略天然互斥，同类策略的效果可能会互相干扰。不同类型的策略通常不会相互干扰。

<img src="./assets/image-20251012145536928.png" alt="image-20251012145536928" style="zoom:33%;" />



**Holdout**是一种考核**系统指标**的机制，**保留10%的用户完全不受实验影响**，可以考察整个部门对业务指标的贡献。



**实验推全**：当实验有效时，会**建立一个推全层**，与其他层正交。



反转实验：**在推全的新层中开一个旧策略的桶，长期观测实验指标**。该实验是为了解决在实验观测到**显著收益**后希望尽快**推全**新策略（为了腾出桶供其他实验使用或基于新策略做后续的开发），具有**滞后性**的指标，有**长期观测的**需求间的矛盾。





<img src="./assets/image-20251012151611287.png" alt="image-20251012151611287" style="zoom: 33%;" />



## 推广搜

推广搜是互联网业务的三驾马车，其中推荐与搜索负责**留住用户，生产流量**，而广告负责**将流量变现**。



三者的共同点如下

1. 在功能架构上，三者都遵循**”先召回粗筛，再由排序精挑细选“**
2. 在数据架构上，三者都遵循**Lambda架构**
3. **许多算法和技术在三个领域是互通的**
4. 推广搜都需要**高度个性化**，搜索不能单纯的返回关键词相关的物料，还要考虑用户画像；广告不能满足用户的需求只会浪费金钱。



推荐和搜索最大的差异在于**用户表达其意图的方式不同**，搜索中，用户通过输入查询语句显示表达其意图

<img src="./assets/image-20251028142300525.png" alt="image-20251028142300525" style="zoom:50%;" />

其中，**t是物料，q是查询语句，u是用户**，模型衡量物料对用户输入的查询的匹配程度，这里考虑了用户信息，因此**不同的用户使用相同的查询语句得到的结果是不同的**。



而推荐系统中用户无需现实表达其意图，推荐系统通过自己的长期观察，猜测用户意图，完成推荐。

<img src="./assets/image-20251028142507412.png" alt="image-20251028142507412" style="zoom:50%;" />



此外，**搜索对推结果的准确性要求更加严格**，例如对多个查询条件搜索时，需要考虑标签的并集

<img src="./assets/image-20251028142740133.png" alt="image-20251028142740133" style="zoom:50%;" />



在推荐中，系统会对结果扩展

<img src="./assets/image-20251028142825182.png" alt="image-20251028142825182" style="zoom:50%;" />



广告是为了流量变现，需要兼顾**用户，广告主，平台**三方的利益，参与方更多更复杂，优化难度更高。

广告关注的目标是**更深层次的转化**，它的**转换链很长**，例如对于电商app广告，一次完成的转化包括：用户点击广告、下载安装App、注册、成功下单。**这会带来严重的延迟反馈问题，能成功转换的样本较为稀疏，建模难度高**。

此外，**广告对指标的精度要求高**，因为广告费用的计算与指标相关，微小的误差会带来损失，而推搜中只要求**相对准确性**——能排名即可。

最后，**广告的候选集会小很多**，因为投放广告存在一定的技术和财力上的门槛。



# 特征工程

现如今的深度学习网络(DNN)，被认为可以当作**万能函数模拟器**，包括但不限于自动模拟特征工程。但越来越多的实践证明DNN的**局限性**——在现实的训练中会出现梯度爆炸，梯度消失，**不同特征受训机会不均衡等问题**，同时DNN会带来**时间上的代价**，因此用**DNN取代特征工程为之过早**。

## 特征提取

物料包括的特征有

+ **物料属性**，在物料入库时就能获取到，在大型推荐系统中，**物料的ID也是重要的特征**
+ **物料的类别和标签，不依赖用户反馈，只通过分析物料内容就能得到，一般由专门的算法获取**
+ **基于内容的Embedding**，用模型（CNN或BERT）的某一层输出作为物料特征
+ **物料的动态画像**，即后验统计数据，按照时间（全生命周期、过去一周）或对象（CTR、平均播放进度、排名）划分
+ **用户给物料反向打标签，将消费过某个物料的用户身上的标签传递并积累到物料上**



用户的特征有

+ 用户的**人口属性**
+ **对用户的行为序列建模**
+ **交叉特征**，交叉的方式有笛卡尔积交叉和内积交叉
+ **偏差特征**，一些推荐的物品由于推荐位置**靠后**使得没有被点击从而被当作负样本，这种误差会诱导模型。<img src="./assets/image-20251028152400032.png" alt="image-20251028152400032" style="zoom:50%;" /> 一种解决方案是从数据入手，比如**更严格地定义正负样本**，以**Above Click**为例，**只有位于被点击物料上方的未点击物料才会被作为负样本**，<img src="./assets/image-20251028152515664.png" alt="image-20251028152515664" style="zoom:50%;" /> 另一种解决方案是从模型入手，在模型中**将偏差作为特征**，**训练时使用真实偏差，预测时将偏差设为一个统一的值**，也被称为伪特征。为了**保证不同的偏差值在预测时不会影响到模型**，偏差特征要**通过一个线性层接入模型，而绝不能和其他正常特征一起喂入DNN**。<img src="./assets/image-20251028152741900.png" alt="image-20251028152741900" style="zoom:50%;" />



# 数据特征的处理

## 处理缺失值

处理缺失值最常规的做法是用**所有样本的均值或中位数**，也可以更**精细**一点，按照**类别划分子样本**取值。



另一种做法是**训练模型预测缺失值**。



如果对**数值特征离散化**，在分桶时，为每个特征增加一个未知的桶，专门存储缺失值。



## 标准化

标准化是为了将不同量纲，不同取值范围的数值特征都压缩到一个数值范围内，使他们具有可比性。最常用的是z-score标准化

<img src="./assets/image-20251028155501864.png" alt="image-20251028155501864" style="zoom:50%;" />



长尾分布的特征需要先变换为正态分布再进行标准化，否则统计的均值和方差会被带偏。

<img src="./assets/image-20251028155611408.png" alt="image-20251028155611408" style="zoom:50%;" />



## 数据平滑和消偏

推荐系统中会遇到样本太少的情况，导致计算结果不可信，例如一个物品只被曝光了一次并被购买，由此计算出的购买率是100%，这显然是不对的。为了消除小样本的负面影响，提高计算结果的置信水平，可以采用威尔逊区间平滑

<img src="./assets/image-20251028160105262.png" alt="image-20251028160105262" style="zoom:50%;" />

<img src="./assets/image-20251028160758020.png" alt="image-20251028160758020" style="zoom:67%;" />

威尔逊区间平滑对简单计算出的比率修正，样本数量越大，修正后的比率越接近远比率。



此外，为了抹平不同细分领域天然存在的偏差，引入了消偏。

以点击率为例，A的点击率优于B，但A处于更显眼的位置，因此点击率不能代表A的内容更出色。为了消除影响可以参考之前的特征工程，或者消偏，采用CoEC

<img src="./assets/image-20251028161229204.png" alt="image-20251028161229204" style="zoom:50%;" />



<img src="./assets/image-20251028161237641.png" alt="image-20251028161237641" style="zoom:67%;" />



## 分桶离散化

分桶有三种方法：

+ **等宽分桶，将特征值域平均划分为N等份**
+ **等频分桶，用N个分位数作为各桶的边界**
+ **模型分桶**，是**简单决策树**拟合特征与目标值，在分桶时，用特征落到的**叶子节点的编号**作为离散化结果



## 类别特征

推荐系统的特征空间主要由**高维的稀疏的类别特征构成**（系统中包含几万个标签，但是每个物品只包含数个）。线上工程中，类别特征可以**减小计算开销**，提升在线预测与预测的实时性。



<img src="./assets/image-20251028162518668.png" alt="image-20251028162518668" style="zoom:50%;" />

以上述公式为例，对于类别特征**只需将非零特征对应的权重相加即可**，省去了乘法运算。



类别特征的维度特别高，再加上实数特征分桶，多维特征交叉，特征空间的维度非常容易上亿，为此出现了**Parameter Server**架构**分散了参数存储和检索的压力**。



推荐系统中类别特征一般通过**Embedding和多特征交叉**增强表达能力。



类别特征中的字符串一般通过**索引表或哈希函数映射到Embedding矩阵，对应一个embedding向**量。

<img src="./assets/image-20251028163542251.png" alt="image-20251028163542251" style="zoom:50%;" />

<img src="./assets/image-20251028163549418.png" alt="image-20251028163549418" style="zoom:50%;" />



# Embedding层

## 概述

Embedding层负责将**稀疏的类别特征映射成一个稠密向量**。

Embedding层还能**概念拆解为特征向量**，目的是**提升推荐算法的扩展能力**，从而能够自动挖掘出那些低频、长尾、小众的模式。将概念拆开，使得推荐算法**从精准匹配转化为模糊查找**。



下述代码展示了如何用库实现简单的Embedding层

```python
import tensorflow as tf
import tensorflow.keras as keras

# 词表
unq_categories=['music','movie','finance','game','military','history']
id_mapping_layer=keras.layers.StringLookup(vocabulary=unq_categories) # 字符拆拿到整数

# Embedding层，六维压缩到四维，多一用于处理不在词表中的单词
emb_layer=keras.layers.Embedding(input_dim=len(unq_categories)+1,output_dim=4)

# 输入
cate_input=tf.constant(['music','movie','finance','game','military','history','None'],dtype=tf.string)
cate_ids=id_mapping_layer(cate_input)

# 得到Embedding后的结果
cate_embeddings=emb_layer(cate_ids)
```



## 共享or独占

共享Embedding是指**同一套Embedding喂入模型的多个地方，发挥多个作用**。例如，双塔模型中用户行为序列特征与物料共享Embedding，

<img src="./assets/image-20251029155235265.png" alt="image-20251029155235265" style="zoom:50%;" />

在交叉特征时，FM算法也会用到。



显而易见，共享Embedding可以**节省存储空间**，因为Embedding矩阵一般都很大。

另一个好处是，共享可以**缓解由于数据不足导致的训练不充分**，对于数据较少的Field可以与邻近的Field共享一个Embedding。



使用独占Embedding有两种情况，一种是避免干扰，另一种是为了更好的进行特征交叉。













# 召回策略

召回是推荐系统的第一个环节，它面对的候选集一般要达到**百万级规模**，所以召回模块的第一要务就是`快`，**为此它可以牺牲一部分精度，只要能找到与用户兴趣比较匹配的物料就可以，而不一定是最匹配的**。

召回模块主要依赖“`离线计算+在线缓存`”模式来实现对上百万规模候选集的快速筛选，离线处理时，因为不知道用户的信息，所以召回模型在结构与特征上都不能出现`用户信息和物料信息的交叉`，这**限制了召回模型**的表达能力，也就制约了召回模型的预测精度。

为了**弥补**精度上的不足，召回模块一般采用`多路召回`的方式，以`数量弥补质量`，每路召回只关注用户或物料信息的一个侧面。

注意，**排序模型并不是删除了交叉信息就能作为召回模型**，二者的应用领域不同，召回是为了**筛选掉不感兴趣的物料**。







## 基于物品的协同过滤 **Itemcf**

### 原理

<img src="./assets/image-20251012153111338.png" alt="image-20251012153111338" style="zoom: 33%;" />



物品之间的相似度基于物品的**重叠受众 **

<img src="./assets/image-20251012153336007.png" alt="image-20251012153336007" style="zoom:50%;" />



如果考虑用户对物品的喜欢程度

<img src="./assets/image-20251012153713581.png" alt="image-20251012153713581" style="zoom: 33%;" />



### 流程

事先做离线计算

- **建立用户->物品的索引**
  - 记录每个用户最近点击，交互过的物品ID
  - 给定用户ID可与找出他近期感兴趣的物品
- **建立物品->物品的ID**
  - 计算物品之间的相似度
  - 对每个物品，索引其最相似的K个物品



线上做交互

1. **给定用户ID，通过用户->物品索引，找到用户最感兴趣的物品列表(last-n)。**
2. **对于last-n列表中每个物品，通过物品->物品的索引，找到top-k相似物品。**
3. **对于取回的nk个相似物品，用公式预估用户对物品的兴趣分数。**
4. **返回分数最高的100个物品，作为该召回通道的推荐结果。**



使用索引，离线计算量大，但是线上计算量小。



## Swing 召回

Swing是ItemCF的**变体**，ItemCF是基于用户重叠来计算相似度的，但是如果用户在一个**小圈子**中(微信群)，圈子中分享的物品被点击或点赞并非出于感兴趣，因此**两个不相干的物品会被错误的认为相似**。

Swing**给用户设置权重**，用户喜欢的物品重叠越多，越说明他们出于一个圈子，依次防止出现小圈子问题。

<img src="./assets/image-20251012160647856.png" alt="image-20251012160647856" style="zoom: 50%;" />



​								<img src="./assets/image-20251012160620392.png" alt="image-20251012160620392" style="zoom: 33%;" /> 





## 基于用户的协同过滤额 UserCF

### 原理

<img src="./assets/image-20251012161615901.png" alt="image-20251012161615901" style="zoom: 33%;" />



计算用户间的相似度，**越热门的物品对计算相似度帮助不大，不能反映出用户的兴趣**，因此需要对物品按照**热度**赋予一个权重。

<img src="./assets/image-20251012162124687.png" alt="image-20251012162124687" style="zoom: 33%;" />



### 流程

事先做离线计算

- **建立用户->物品的索引**
  - 记录每个用户最近点击，交互过的物品ID
  - 给定用户ID可与找出他近期感兴趣的物品
- **建立用户->用户的ID**
  - 计算用户之间的相似度
  - 对每个用户，索引其最相似的K个用户



线上做交互

1. **给定用户ID，通过用户->用户索引，找到最相似的k个用户top-k。**
2. **对于top-k列表中的用户，通过用户->物品的索引，找到last-n。**
3. **对于取回的nk个相似物品，用公式预估用户对物品的兴趣分数。**
4. **返回分数最高的100个物品，作为该召回通道的推荐结果。**







## 矩阵补充

### 原理

<img src="./assets/image-20251013142731944.png" style="zoom:67%;" />



模型的输入为(用户ID，物品ID，兴趣分数)的集合，兴趣分数是系统记录的，如

+ 曝光但是没有点击，0分
+ 点击，点赞，收藏，转发，各算1分
+ 分数最低是0最高是4



**模型将物品ID和用户ID做embedding，映射为向量，两个向量的内积作为用户对物品的兴趣分数。**



让模型拟合观测的真实兴趣分数，**得到两个Embedding层的参数**，即一个矩阵，矩阵的**列**表示用户/物品。



数据集可以看作是给定了下述的矩阵，模型通过拟合得到其他位置的兴趣分数，最终做推荐。

<img src="./assets/image-20251013143122154.png" alt="image-20251013143122154" style="zoom:67%;" />



矩阵补充的实践效果并不好

1. **没有利用物品和用户的属性**
2. **负样本的选取方式不对**
3. **做训练的方法不好，如损失的计算，使用内积**



### 线上召回

1. 训练得到的Embedding矩阵A和B，A中每一列是用户，B中每一列是物品。
2. 把矩阵A的列存储到**字典**中，B用**其他方式**存储。
3. 把**用户ID**作为key查找得到**用户向量a**
4. 第i号物品的embedding记作$b_i$，**返回能使内积<a,$b_i$>最大的k个物品**



最后一步中，如果枚举所有物品，**时间复杂度会正比于物品数量**，工业中一般采用**近似最近邻查找**，**最近邻的标准有欧氏距离，向量内积，余弦相似度**。



首先将物品**按照最近邻划分为多个区域**，每个区域用一个**向量**表示，同时**建立区域内点到向量的映射**。

<img src="./assets/image-20251013145221922.png" alt="image-20251013145221922" style="zoom:50%;" />



接下来计算**用户向量与区域向量的距离**（取决于最近邻的标准），找出**最近**的向量；再计算该向量**区域内所有点与用户向量的距离**，得到最近的K个。

<img src="./assets/image-20251013145602143.png" alt="image-20251013145602143" style="zoom:50%;" />



这些改进大大减小了枚举的次数。



## 双塔模型

### 原理

双塔模型可以看作是矩阵补充模型的**升级**版，双塔指的是**用户塔和物品塔**



输入考虑了物品和用户的属性



<img src="./assets/image-20251013152519773.png" alt="image-20251013152519773" style="zoom:50%;" />



<img src="./assets/image-20251013150143306.png" alt="image-20251013150143306" style="zoom: 50%;" />



使用**余弦相似度**作为兴趣分数

<img src="./assets/image-20251013150337633.png" alt="image-20251013150337633" style="zoom:50%;" />



在训练好神经网络参数后，**物品表征**事先存储在**向量数据库**中，**线上不做推理**，因为物品特征较为**稳定**。**用户特征**只需做一次**线上推理**。

推荐时按照**近似最近邻查找**，得到能与用户向量计算出距离最近的物品向量。



双塔模型属于**后期融合**——底层之前不对物品和用户特征做融合，线上计算量小。



### 训练

Pointwise：**独立看待每个正样本和负样本，做简单的二元分类**。对于正样本估计兴趣分数**接近1**，负样本则是**-1**，训练中正负样本的比例为**1:2或1:3**（经验）



Pairwise：**每次取一个正样本和负样本（两个样本共享神经网络参数）**，分别计算用户和正样本的余弦相似度和负样本的余弦相似度，**鼓励前者大于后者，且差越大越好**。

<img src="./assets/image-20251013153440477.png" alt="image-20251013153440477" style="zoom:50%;" />





<img src="./assets/image-20251013153520537.png" alt="image-20251013153520537" style="zoom:50%;" />



常用的损失函数有

<img src="./assets/image-20251013153609276.png" alt="image-20251013153609276" style="zoom:50%;" />







Listwise：**每次取一个正样本和多个负样本**



计算出余弦相似度后用**Softmax**激活函数映射到0和1之间，**鼓励$S^{+}$为1，$S{-}$为0**

<img src="./assets/image-20251013153818735.png" alt="image-20251013153818735" style="zoom:67%;" />



损失函数用交叉熵

<img src="./assets/image-20251013154003834.png" alt="image-20251013154003834" style="zoom:50%;" />



### 正负样本

正样本的选取很简单，**使用曝光且有点击的物品**即可，需要**平衡**冷门物品和热门物品，对**前者过采样对后者降采样**。



负样本的来源有：没有被召回，被召回但是被排序截断，被曝光但是没有被点击。



简单负样本：

- 未被召回的物品大概率是用户不感兴趣的，同时**数量约等于全体物品**，，可以在**全体物品中抽样**作为负样本，每个样本**被抽取的概率与点击次数的0.75次幂成正比**，**目的是打压热门物品**。
- 按照**(用户，点击过的物品)**选取一个batch，batch中其他**n-1**个物品对于一个用户就是负样本。但是**一个物品出现在batch中的概率正比于它的点击次数**，如果按照batch的方法划分负样本，那么**物品成为负样本的概率正比于点击次数**，会导致**热门物品**成为负样本的概率**变大**。修正方法是在**训练时调整兴趣分数为**<img src="./assets/image-20251013162338781.png" alt="image-20251013162338781" style="zoom:50%;" />

<img src="./assets/image-20251013162029477.png" alt="image-20251013162029477" style="zoom:50%;" />





困难负样本是**被排序淘汰的物品**	



训练时**混合两种样本，对半开**



**被曝光但是没有被点击不应该作为负样本**，它会导致模型效果下降，因为召回阶段是为了**区分不感兴趣和感兴趣的样本**，而不是**比较感兴趣的程度**，被曝光的物品已经属于比较感兴趣的物品了，甚至可以用作**正样本**，将其作为负样本不会带来效益。



### 模型更新

全量更新：在今天凌晨，用**昨天的数据在昨天的模型参数基础上做训练**，并更新**物品向量**。

增量更新：每隔**几十分钟**更新模型参数（**只更新ID Embedding参数**），需要**实时收集线上数据**。



一般系统会**结合使用**两种更新方式。



### 自监督

推荐系统的**头部效应**严重：少部分物品占据大部分点击，大部分物品的点击次数不高。**因此高点击物品的表征学的好，长尾物品的表征学得不好**。



为了更好的学习长尾物品的向量表征，对物品做**自监督学习**，训练物品塔



<img src="./assets/image-20251014143636070.png" style="zoom: 50%;" />

**同一物品**经过特征变换后从物品塔得到的特征向量应该有**高相似度**，**不同物品**的特征向量应该有**低相似度**。



特征变换的方法有：

+ Random Mask，**随机选取离散的特征置**为default，相当于丢弃
+ Dropout，对**多值离散特征**随机丢弃其中**50%**
+ Complementary，把特征随机分为**两组**，填充为 <img src="./assets/image-20251014144029731.png" alt="image-20251014144029731" style="zoom:33%;" />的形式
+ Mask关联的特征，**离线计算出特征间的互信息**，**随机选一个特征作为种子**，将**其和与其最相关的k/2个特征丢弃**。这种方法效果最好，但是方法复杂，实现难度大，不好维护。



训练模型时，从**全体物品中均匀抽样**，得到一个batch，做两类特征变换输出两组向量，每个物品的损失函数如下

<img src="./assets/image-20251014144855326.png" alt="image-20251014144855326" style="zoom:50%;" />

最后做梯度下降最小化损失函数

<img src="./assets/image-20251014144944562.png" alt="image-20251014144944562" style="zoom:50%;" />







对点击做随机抽样，对全体用户做均匀抽样得到两个batch，同时训练双塔和自监督

<img src="./assets/image-20251014145523697.png" alt="image-20251014145523697" style="zoom:50%;" />

## Deep Retrieval

把物品表征为路径，线上查找用户最匹配的路径。



### 索引

下图中路径的深度为3，宽度为K

<img src="./assets/image-20251014145925757.png" alt="image-20251014145925757" style="zoom:67%;" />



预先建立**物品到路径**和**路径到物品**的索引，两者都是**一对多**的关系



### 预估用户对路径的兴趣



<img src="./assets/image-20251014154739306.png" alt="image-20251014154739306" style="zoom:50%;" />

对于路径[a,b,c]，输入用户特征x，经过神经网络和Softmax层得到p1，选择节点a；对节点a做**embedding**和用户特征**拼接**，输入到神经网络和Softmax中得到p2，选择其中的节点b...



最后的分数如下

<img src="./assets/image-20251014155042731.png" alt="image-20251014155042731" style="zoom:50%;" />



### 线上召回

1. **给定用户特征，用beam search召回一批路径。**
2. **利用索引，召回一批物品**
3. **对物品打分和排序，选出一个子集，打分的方法不固定**



beam search是一种**贪心**思想，用于选取**最优**的路径，**但不保证最优**。



以beam size=4为例

**对第一层，选出分数最高的四个节点**

<img src="./assets/image-20251014160039387.png" alt="image-20251014160039387" style="zoom:50%;" />



考虑从**四个节点出发**经过的**4K条路径**中**分数最大的4个**

<img src="./assets/image-20251014160109521.png" alt="image-20251014160109521" style="zoom:50%;" />

从**上一步选择的节点出发**，找出分数最大的4条路径

<img src="./assets/image-20251014160153860.png" alt="image-20251014160153860" style="zoom: 33%;" />



### 训练

**同时学习神经网络参数和物品表征**



对于神经网络参数，只用正样本，即click(user,item)=1。将点击过的物品表述为多条路径，最大化用户对路径的分数

<img src="./assets/image-20251014160745764.png" alt="image-20251014160745764" style="zoom:67%;" />







定义物品与路径的**相关性**



<img src="./assets/image-20251014172334738.png" alt="image-20251014172334738" style="zoom:67%;" />



选出相关性**最大**的路径作为物品的表征

<img src="./assets/image-20251014172455509.png" alt="image-20251014172455509" style="zoom:50%;" />



显然，要使物品表征出的**路径相关分数最大**，同时为了避免**过多的物品被一条路径表征**，引入正则项惩罚：**路径表征的物品数的4次幂**。

<img src="./assets/image-20251014173942887.png" alt="image-20251014173942887" style="zoom:67%;" />

<img src="./assets/image-20251014173951658.png" alt="image-20251014173951658" style="zoom: 67%;" />





更新时，从未被选择的路径中，选出使损失函数和正则项和最小的最为新的路径。这一步理解不够

<img src="./assets/image-20251014174012528.png" alt="image-20251014174012528" style="zoom:50%;" />



## 其他召回

**Geohash召回**，根据用户的**地理位置**召回，取回该地点**最新的K篇优质笔记**，**不考虑个性化**。



Geohash将**二维的经纬度坐标**转换为带有**前缀性质索引性质的一维坐标**。在一定程度上可以保证，两个Geohash的**公共前缀**越长，其所对应的区域就越近。



首先对经度纬度**二分**

- 第一轮：经度范围为 -180°~180°，可以拆分成 -180°~0° 以及 0°~180° 两部分，如果经度从属于前者，则令首个 bit 位取 0，否则令首个 bit 位取 1；
- 第二轮：-180°~0° 可以拆分为 -180°~-90° 和 -90°~0°，如果经度从属于前者，则令第二个 bit 位取 0，否则令第二个 [bit 位](https://zhida.zhihu.com/search?content_id=231481591&content_type=Article&match_order=4&q=bit+位&zhida_source=entity)取 1；0°~180° 可以拆分为 0°~90° 和 90°~180°，前者令第二个 bit 位取 0，后者令第二个 bit 位取 1
- 第 3 ~ N 轮：重复上述步骤的思路，最终递归二分，将经度表示成一个由二进制数字组成的字符串

<img src="./assets/v2-f49beb4f7d33b5063cffc7a54a3fef08_1440w.jpg" alt="img" style="zoom:50%;" />



将转换后的经纬度按照**经度字符串+纬度字符串依次交错排列**的形式组合，最后按照**5个一组划分**按**base32**规则映射，得到最后的字符编码。





**同城召回**，按照**城市**召回最新的K篇优质笔记，**不考虑个性化**。



**缓存召回**，**复用前n次推荐精排的结果**，因为缓存**有限**，需要**退场**机制。



此外还有作者召回。

### 曝光过滤&Bloom Filter

如果用户**看过**某个物品，则不再把该物品**曝光**给该用户。对于**每个用户记录曝光给他的物品(一定时间内)**，对于每个被召回的物品，判断它是否已经给该用户曝光过，**排除掉曾经曝光过**的物品。



**暴力对比**召回的物品是否在曝光记录中，会带来较大的**计算开销**。



Bloom Filter是一种**数据结构**，**判断一个物品ID是否在已曝光的物品集合中**。返回结果为**No**，那么**一定不在**；返回结果为**Yes**，该物品**可能在集合中**(有概率误判)。因此使用Bloom Filter**一定能过滤掉已曝光的物品，但是会导致未曝光的物品被剔除**。该结构的缺点在于，**不支持删除物品**。



Bloom Filter将**物品映射为m维向量**，使用**k个哈希函数**。初始化时，将**已曝光物品**使用**哈希函数**映射到m位上的k个位置，将这些位置设置为**1**，被**重复**映射的位置**忽略**。对被召回的同样使用k个哈希函数映射，如果**全部位置都为1**，说明该物品**被曝光**，反之未被曝光。

<img src="./assets/image-20251015150008218.png" alt="image-20251015150008218" style="zoom:50%;" />





曝光物品集合大小为**n**，二进制向量维度为**m**，使用**k**个哈希函数	

<img src="./assets/image-20251015150307310.png" alt="image-20251015150307310" style="zoom:67%;" />



人为设定可容忍的误伤概率后，有

<img src="./assets/image-20251015150417274.png" alt="image-20251015150417274" style="zoom:50%;" />



# 排序



粗排：从召回通道召回的物料中再次筛选，用于**快速过滤低质量候选**。

精排：精排的任务是从上有层层筛选出的千级规模的比较符合用户兴趣的物料中，挑选出几十个最合用户品味的物料。精排的侧重点是**提升预测精度**，看重**物料信息和用户信息的交叉特征**。

## 排序特征

- 用户画像：用户ID，人口统计学属性，账号信息，感兴趣的类目
- 物品画像：物品ID，发布时间，内容信息，类目，关键词
- 用户统计特征：用户不同时间段的/对不同类目的统计量
- 物品统计特征：物品不同时间段/按用户特征划分的统计量，发布者统计特征
- 场景特征：用户位置，当前时间，登录设备



物品特征**稳定**，用户特征**较为稳定**，统计特征**动态变化**。



## 多目标排序模型

多目标排序属于**前期融合**，比后期融合准确度高，**线上做推理的代价大**，有n个物品就要做n次推理。

<img src="./assets/image-20251015152419833.png" alt="image-20251015152419833" style="zoom:50%;" />



目标是点击/点赞/收藏/转发，如果有对应的行为，那么值就是1，反之为0。使用**交叉熵**计算每个指标的损失，最后汇总起来，做梯度下降。

<img src="./assets/image-20251015153543665.png" alt="image-20251015153543665" style="zoom:50%;" />



模型中以**样本是否被点击过**来判断正负样本，系统中负样本的数量会**远远大于**正样本的数量，模型不应该把大量的时间花费到学习无用的负样本上，因此会对负样本做**降采样**——保留一小部分负样本，让两类样本数量平衡。



由于负样本减少，**预估出的点击率会大于真实点击率**，因此需要做预估值校准



**真实点击率：**
$$
p_{\text{true}} = \frac{n_+}{n_+ + n_-}
$$
**预估点击率：**
$$
p_{\text{pred}} = \frac{n_+}{n_+ + \alpha n_-}
$$
**校准公式：**
$$
p_{\text{true}} = \frac{\alpha \, p_{\text{pred}}}{(1 - p_{\text{pred}}) + \alpha \, p_{\text{pred}}}
$$

## Multi-gate Mixture-of-Experts MMoE

<img src="./assets/image-20251015154646850.png" alt="image-20251015154646850" style="zoom:50%;" />

中间的神经网络被称为**专家**，它们不共享参数，专家的数量在不同的系统中不固定。



两侧的**神经网络和激活函数**用于求解一个**权重向量**，权重用于和专家的结果**加权平均**求解指标，用多少个指标就有多少个权重向量。

<img src="./assets/image-20251015154929619.png" alt="image-20251015154929619" style="zoom:50%;" />



系统中的Softmax激活函数可能出现**极化现象**：**Softmax输出值中的⼀个接近1，其余接近0**。这会导致只有一个专家的推理结果被使用。

<img src="./assets/image-20251015155012310.png" alt="image-20251015155012310" style="zoom:50%;" />

为了解决这个问题，**训练时对Softmax输出的每个向量值以10%的概率dropout**。	





## 视频播放建模

视频排序的依据额外包括**播放时长和完播率**。



实践证明，**直接用回归拟合播放时长的效果不好**，设关于播放时长的**全连接层**的输出为**z**，播放时长为**t**，定义$p = \frac{\exp(z)}{1 + \exp(z)}，y = \frac{t}{1 + t}$，**损失函数为 $CE(y, p) = y \cdot \log p + (1 - y) \cdot \log(1 - p)$。**在**预测**时输出**exp(z)**作为播放时长的指标。

<img src="./assets/image-20251015161558937.png" alt="image-20251015161558937" style="zoom: 50%;" />





完播率可以用**回归或者分类**方法求解。回归方法：如果视频长度为10mins，播放了6mins，那么完播率就是0.6，使用交叉熵函数拟合；分类，工程师**自定义**正负样本，如播放时间大于80%记为正样本。



得到**完播率后不能直接用于融分公式**，因为**长视频**的完播率显然低于**短视频**，需要做**调整**保证公平。

<img src="./assets/image-20251015162023205.png" alt="image-20251015162023205" style="zoom: 50%;" />



## 粗排三塔模型

**计算开销介于双塔模型和精排模型之间。**

<img src="./assets/image-20251016133002023.png" alt="image-20251016133002023" style="zoom: 50%;" />

- 用户塔很大，线上只做一次计算。
- 物品塔较大，系统会**缓存**物品塔的输出，因此物品塔可以**避免大部分推理**。
- 交叉他较小，用于计算统计特征和交叉特征，有n个物品就要做n次推理。

模型的主要开销集中在**上层**



粗排阶段也考虑多样性，首先会将**兴趣分数最高**的200个物品送入精排，随后对剩下的物品计算多样性分数，取出其中**兴趣分数+多样性分数最高**的300个送入到精排。







# 交叉结构

## Factorized Machine FM

传统的线性模型中只能表示单个特征对目标的影响


$$
\hat{y} = w_0 + \sum_i w_i x_i
$$
FM在此基础上加入**特征交互项**，但用**向量内积**代替直接的交叉参数：
$$
\hat{y} = w_0 + \sum_i w_i x_i + \sum_{i<j} (v_i^\top v_j) x_i x_j
$$
<img src="./assets/image-20251016134334137.png" alt="image-20251016134334137" style="zoom:67%;" />



使用向量内积大大**减小了参数的数量**。



## 深度交叉网络 DCN

召回和排序模型中的神经网络可以采用DCN，效果比使用全连接网络要好。**DCN是交叉神经网络和全连接网络组成的**

<img src="./assets/image-20251016140528620.png" alt="image-20251016140528620" style="zoom:50%;" />





交叉神经网络由**交叉层**组成

<img src="./assets/image-20251016135709932.png" alt="image-20251016135709932" style="zoom:50%;" />

<img src="./assets/image-20251016140039766.png" alt="image-20251016140039766" style="zoom:50%;" />

Hadamard product是**逐元素相乘**







交叉神经网络结构如下

<img src="./assets/image-20251016140326738.png" alt="image-20251016140326738" style="zoom:50%;" />



## LHUC网络结构

与DCN类似，但是只能用于**精排**，该网络结构最初是用于语音识别。

<img src="./assets/image-20251016141245582.png" alt="image-20251016141245582" style="zoom:50%;" />

其中神经网络的结构是**多个全连接层+sigmod*2**



## SENet & Bilinear Cross

SENet对**特征加权**

<img src="./assets/image-20251016142811356.png" alt="image-20251016142811356" style="zoom:50%;" />



假设有m个特征，经过**Embedding**后得到m个k维向量（**向量的大小可以不同**），经过平均池化，全连接+ReLU，全连接+Sigmoid后得到m维的**权重向量**，将权重向量的值**乘到[m,k]的输入对应行的每个元素上**得到加权后的结果。



特征交叉的方法有**内积和Hadamard乘积**

 <img src="./assets/image-20251016143418563.png" alt="image-20251016143418563" style="zoom: 50%;" />



**Bilinear Cross是一种更先进的特征交叉方法，在上述两种方式上引入了额外的参数矩阵**

<img src="./assets/image-20251016143636256.png" alt="image-20251016143636256" style="zoom:50%;" />

<img src="./assets/image-20251016143644579.png" alt="image-20251016143644579" style="zoom:50%;" />



## FiBiNet

FiBiNet是**结合了Bilinear cross和SENet的模**

<img src="./assets/image-20251016143854254.png" alt="image-20251016143854254" style="zoom:50%;" />



# 用户行为序列建模

## LastN特征

LastN特征是**用户特征**中的一种：**把用户最近交互过的N个物品ID做Embedding得到n个向量后取平均**。

<img src="./assets/image-20251016145502179.png" alt="image-20251016145502179" style="zoom:50%;" />

适用于**双塔模型，三塔模型，精排模型**

## Din模型

Din使用**加权平均**代替平均，即**注意力机制**，**权重**就是候选物品与LastN物品间的**相似度**。

<img src="./assets/image-20251016150517956.png" alt="image-20251016150517956" style="zoom:50%;" />



Din适用于**精排模型**，因为注意力机制需要用到**物品的特征**，在**双塔和三塔模型中用户塔是看不到物品的**。



## SIM模型

DIN模型的计算量与**N的大小**有关，因此只会保留最近的**几百个物品**，只会导致模型**关注短期兴趣，遗忘长期兴趣**。



SIM模型在DIN模型的基础上**保留更大的N**，计算时**快速排除**掉与**候选物品无关**的LastN物品，**降低计算量**。



SIM模型第一步做**查找**

+ **Hard Search，根据候选物品的类目，保留类目相同的即可，简单快速无需训练。**
+ **Soft Search，物品做Embedding，把候选物品做query，使用K近邻查找。效果更好，但是实现更复杂。**



接下来**计算相似度做加权平均**。

<img src="./assets/image-20251016152645480.png" alt="image-20251016152645480" style="zoom:50%;" />



实现中，会将LastN物品与**交互时间的Embedding向量**拼接，因为SIM记录用户的**长期行为**，时间越久远重要性越低。



# 多样性

重排的主要目的不是过滤和筛选， 而是**调整精排结果的顺序**，将相似内容**打散**，以保证用户在一屏之内看到的推荐结果丰富而多样。

## 多样性的度量

基于**物品属性**标签。

- 类目、品牌、关键词……，以类目为例，计算出多级类目的**相似度后做加权和**，权重按照**经验**设置



基于物**品向量**表征。

- **用召回的双塔模型学到的物品向量（不好）**，由于物品具有**长尾特征**，**物品塔不能很好的学到物品的表征**。
- **基于内容的向量表征（好）**。使用**CV和NLP**的技术提取**物品内容**的特征与其他物品计算相似度，但是这一过程**缺少标注数据**无法训练网络。目前主要通过**CLIP**预训练方法——**对于图片和文本的二元组，预测图文是否匹配**。

​                           <img src="./assets/image-20251016155843248.png" alt="image-20251016155843248" style="zoom:50%;" /><img src="./assets/image-20251016155851871.png" alt="image-20251016155851871" style="zoom:50%;" />



多样性算法用在**排序的后处理**中，精排的后处理称为**重排**。

<img src="./assets/image-20251016155925435.png" alt="image-20251016155925435" style="zoom:50%;" />



## Maximal Marginal Relevance MMR

从精排的物品中选出具有多样性的k个物品，那么既要考虑**精排分数**也要考虑**相似度分数**。



MMR定义公式

<img src="./assets/image-20251017141446959.png" alt="image-20251017141446959" style="zoom:50%;" />

其中S是**已选中物品的集合**，每一轮在**未选中物品**中取出**MR分数最大**的物品作为已选物品。



实际应用中，随着S集合的**增大**，物品的相似度必然会**趋近1**，MMR就会失效。因此将S换为**最近选出的W个物品**，用**滑动窗口**维护，直观上理解——第30个物品没必要和第1个物品保证不相似。

![image-20251017141731079](./assets/image-20251017141731079.png)



## 重排规则

+ 最多连续出现K个同类物品
+ 每K个物品最多出现1个推广的物品
+ 前t个物品最多出现k个同种物品



重排会**结合MMR与规则**，在满足规则的前提下**最大化MR分数**，每一轮会提取出满足规则的选物品，再用公式计算分数。	



## DPP

一组向量 **$\mathbf{v}_1, \cdots, \mathbf{v}_k \in \mathbb{R}^d$** 可以确定一个 **$k$ 维超平行体**（二维中是平行四边形，三位中是平行六面体）：
$$
\mathcal{P}(\mathbf{v}_1, \cdots, \mathbf{v}_k) = \{ a_1 \mathbf{v}_1 + \cdots + a_k \mathbf{v}_k \mid 0 \le a_1, \cdots, a_k \le 1 \}.
$$
要求**k<=d**，因为三维空间中可以有二维超平形体，但反过来不行。



如果向量之间**线性相关**，那么超平形体的体积为**0**。把**物品表征为向量**，它们形成的**超平形体的体积**可用于**衡量多样性**——**体积越大，多样性越好**。



将向量组成**矩阵**，那么**体积可以用矩阵的行列式**表示：$\det(V^{T}V) = \operatorname{vol}(\mathcal{P}(\mathbf{v}_1, \cdots, \mathbf{v}_k))^2.$

<img src="./assets/image-20251017150741802.png" alt="image-20251017150741802" style="zoom:50%;" />



**DPP**，行列式点过程，在候选集中选出**相似度不高**的子集
\[
\underset{S: |S| = k}{\text{argmax}} \log \det(V_S^T V_S)
\]

Hulu将DPP应用到推荐系统中
\[
\underset{S: |S| = k}{\text{argmax}} \theta \cdot (\sum_{j \in S} \text{ reward}_j) + (1 - \theta) \cdot \log \det(A_S)
\]
设A为\(k \times k\)的矩阵，它的\((i,j)\)元素为\(a_{ij} = \boldsymbol{v}_i^T \boldsymbol{v}_j\)，给定k个d维的向量，计算矩阵A需要**O($k^2d$)**的时间。



想要找出**集合S**是一个**NP-hard**的问题，只能通过**贪心算法**近似求解，**每一轮从未选物品中找出一个分值最大的物品作为已选物品**，候选物品添加到矩阵A中就是**多出一行和一列**。
$$
\underset{i \in \mathcal{R}}{\text{argmax}} \ \theta \cdot \text{reward}_i + (1 - \theta) \cdot \log \det(\boldsymbol{A}_{\mathcal{S} \cup \{i\}})
$$
上述公式中，**计算行列式需要O($n^3$)**，一轮要遍历R个物品，一共要选出K个，那么时间复杂度近似
$$
O(|\mathcal{S}|^3 \cdot |\mathcal{R}| \cdot k) = O(nk^4)
$$


最后总的时间复杂度为**$$O(n^2d + nk^4)$$**



推荐系统的重排时间一般只有10ms，对于上述系统的时间复杂度显然**超出**，Hulu提出利用**Cholesky分解**加速计算，仅需 **\(O(n^{2}d + nk^{2})\)** 的时间。Cholesky分解**$\boldsymbol{A}_{\mathcal{S}} = \boldsymbol{L}\boldsymbol{L}^T$**，其中 $\boldsymbol{L}$ 是**下三角矩阵**。$\boldsymbol{A}_{\mathcal{S}}$ 的行列式为 **$\det(\boldsymbol{A}_{\mathcal{S}}) = \det(\boldsymbol{L})^2 = \prod_i l_{ii}^2$**，添加一个物品的变换可以**快速反应**在$\boldsymbol{L}$上。



DPP也使用**滑动窗口**并可以**结合规则**使用。



# 物品冷启动

## 评价指标

冷启动：**系统缺乏足够的信息，无法给用户或物品提供有效的推荐。**



冷启动是推荐系统中**比较复杂**的部分，其考察的指标比其他部分要**多**

+ 作者测指标，目标是促进发布，增大内容池
  + 发布渗透率，当日发布人数/日活人数
  + 人均发布量，当日发布笔记数/日活人数
+ 用户侧指标
  + 新物品的消费指标，点击率交互率，按照曝光数区分
  + 大盘消费指标，系统总体的消费时长和日活，月活
+ 内容侧指标
  + 高热物品占比



## 简单的召回通道

**ItemCF**不适用于物品冷启动，因为新物品**缺少用户交互**。



新物品的**ID Embedding还没学好**，导致**双塔模型**的效果不好，在实现中有两种改进方案

+ **所有新物品共享一个default Embedding**
+ 查找内容**最相似**物品，取**曝光量最高**，对这些物品的**Embedding取平均**作为新物品的Embedding



类目召回，**系统维护类目->物品列表(按时间倒序)**，用类目索引召回**最新的笔记**。与之类似的还有**关键字召回**。这两种召回方法的缺点在于：**只对刚刚发布的物品有效，弱个性化，不够精准。**





## 聚类召回

**聚类索引**：一个物品发布时，用神经网络映射到特征向量，**从聚类中心中找到最相似的cluster**，将该物品添加到**cluster->物品ID列表中(按时间倒排)**



线上召回时：给定用户ID，取回**last-n**物品作为种子笔记，每个种子笔记映射到向量并找出**最相似的cluster**，取回其中**最新的m个物品**。



模型如下

<img src="./assets/image-20251018143210376.png" alt="image-20251018143210376" style="zoom:50%;" />



神经网络**共享参数**，接受物品的**图文**内容，要求**种子物品和正样本相似度大，和负样本相似度小**，损失函数如下

Triplet hinge loss: $$L(\mathbf{a},\mathbf{b}^+,\mathbf{b}^-) = \max\{0, \cos(\mathbf{a},\mathbf{b}^-) + m - \cos(\mathbf{a},\mathbf{b}^+)\}.$$

Triplet logistic loss: $$L(\mathbf{a},\mathbf{b}^+,\mathbf{b}^-) = \log(1 + \exp(\cos(\mathbf{a},\mathbf{b}^-) - \cos(\mathbf{a},\mathbf{b}^+))).$$



其中正样本有两种来源：

1. **人工标注相似度**
2. **筛选出高曝光且二级类目相同的物品，用ItemCF选取最相似的物品**



负样本从**全体物品中随机选择**，需要满足物品质量高，内容丰富。



## Look-Alike人群扩散

Look-Alike最初是用于广告系统中的，通过用户画像筛选出对物品感兴趣的**少量种子用户**，再将物品**扩散到和种子用户相似的用户群体**上。

<img src="./assets/image-20251018144720378.png" alt="image-20251018144720378" style="zoom:50%;" />



在推荐系统中，对于新物品，如果有**交互行为**则认为交互的用户属于该物品的**种子用户**，对**种子用户的ID Embedding取平均，作为物品的一个特征**。每当有用户交互，**更新**物品的特征向量。

<img src="./assets/image-20251018144832149.png" alt="image-20251018144832149" style="zoom:50%;" />



特征会存储到**向量数据库**中，推荐时将用户的ID Embedding输入到数据库中做**最近邻查找**取回物品

<img src="./assets/image-20251018145225555.png" alt="image-20251018145225555" style="zoom:50%;" />



## 流量调控

对于新物品扶持可以**促进发布，增大内容池，挖掘优质物品**。



工业界中系统只会**推荐年龄<30天的物品**，对于**<=24小时**的物品**扶持使其曝光量大于1/30**。



流量调控的做法是**提权**，干涉**粗排和重排**环节，给新物品提权。这是一种**容易实现且投入产出比好**的方法，但是**提权系数不好调控**，很难控制曝光量容易造成过度曝光或不充分曝光。



**保量**：**不论笔记质量高低，都保证24小时内获得100次曝光**，在原有的提权系数上**乘以额外的系数**，该系数通过**目标时间，目标曝光量，发布时间，发布曝光量**计算，最终形式如下。达到了曝光目标后和旧物品做**公平竞争**。

<img src="./assets/image-20251018152030054.png" alt="image-20251018152030054" style="zoom:50%;" />



**保量的提权系数也不好做**，同时线**上环境变化**会引起提权系数的**重调**。



注意，采用保量时，不能调整权重系数为**一个非常大的数**快速到达目标曝光量，这会导致物品被推荐给**不合适**的用户，使得**物品的数据(点击率，点赞率)偏低，最终物品会收到系统打压，无法称为热门物品**。



**差异化保量**：不同的物品有**不同的指标**，优质的物品获得**更高的保量指标**。可以用多模态，或作者的历史数据来判断内容的好坏。



## AB测试

冷启动的AB测试需要考虑两个指标——**作者侧指标和用户侧指标**



用户侧指标观察到的diff推全后不准确。

<img src="./assets/image-20251018155752928.png" alt="image-20251018155752928" style="zoom:50%;" />





作者侧指标

<img src="./assets/image-20251018160142825.png" alt="image-20251018160142825" style="zoom:50%;" />

- **假设新老笔记走不同的流量通道，增大实验组权重时，会导致新老笔记间竞争流量，推全后结果不准确**
- **假设行老笔记自由竞争，不仅会出现上述问题，新老笔记间也会竞争。**



<img src="./assets/image-20251018160211990.png" alt="image-20251018160211990" style="zoom:50%;" />

上述方案中，虽然**新物品不会抢流量**，但是**新老物品**依然会抢流量，同时**物品池小了一半**会导致用户侧指标下降。



<img src="./assets/image-20251018160328094.png" alt="image-20251018160328094" style="zoom:50%;" />

不会出现抢流量的情况，但是物品池减小的问题没有解决。



# 张指标的方法

## 评价指标

**日活用户和留存**是最核心的指标，**UGC**的应用会将**发布量和发布渗透率**也作为核心指标。



工业界常用**LT7和LT30**衡量留存，LT7指的是用户在某一天登陆了应用，**包括这一天的未来一周内的登录次数/7**就是用户这一天的LT7值。所有**用户LT7值的平均**就是系统某一天的LT7结果。



## 召回

推荐系统有几十条召回通道，召回总量是**固定**的，总量越大，指标越好，粗排计算量越大。



在召回方面可做的改进有：

1. **双塔模型**
   1. **优化正负样本**
   2. **优化网络结构**
   3. **优化训练方法**
2. **I2I(基于相似物品做召回)，使用多种计算相似度的方法做召回**
3. **添加小众的召回模型**



## 排序模型

排序模型的改进分为：

1. 精排模型的改进
   1. 基地加宽加深，计算量更大，预测更准确
   2. 做自动的特征交叉，Bilinear和LHUC
   3. 特征工程，添加统计特征和多模态特征
   4. 增加新的预估目标，加入到融分公式中
   5. 纠正position bias
2. 粗排模型的改进
   1. 使用更复杂的模型
   2. 蒸馏精排训练粗排，让二者更一致
3. 用户行为序列建模
   1. 增加序列长度
   2. 筛选方法
   3. 使用ID以外的特征
4. 在线学习
   1. 既做全量更新也做增量更新
5. 老汤模型(老模型泡的久，新模型暂时无法超过)
   1. 随机初始化网络层和Embedding层参数，比较两个模型的效果
   2. 用老模型的结果蒸馏新模型，加快新模型训练。



## 多样性

双塔模型的多样性

1. 线上做召回时，往**用户向量**中添加**随机噪声**，**用户兴趣越窄，噪声越强**。添加噪声使得召回的物品更多样，可以提升系统核心指标。
2. 用户的**LastN**向量中保留最近的r个，对**其余的n-r个随机抽样**(均匀或非均匀)，注入多样性，**提升召回的结果，同时n可以取得比较大**。



**U2I2I**中扩大用户**最近交互过的物品的范围**，按照**类目**做**非均匀取样**，让类目平衡。一方面，**类目更平衡，多样性更好**。另⼀方面， **n可以更大， 覆盖的类目更多**。



## 特殊人群

特殊人群指的是**新用户和低活用户**



对特殊人群要构造**特殊的内容池**，因为**特殊人群的行为很少，召回效果不好**

+ 根据物品获得的交互次数，交互率选择优质物品
+ 做因果判断，判断物品对人群留存率的贡献，根据贡献率选物品

内容池**定期更新**，**排除交互率低和失去时效性的物品**。通常使用双塔模型从特殊内容池里做召回，每个新用户需要单独的训练模型，老用户共享一个模型。



排序时**避免推送新物品和广告**给新用户，保证体验。



低活用户的人均点击量很小，没有点击就不会有进一步交互，因此在**融分公式**中**提高预估点击率的权重**。另外，会**保留**几个曝光坑位给**预估点击率最高**的物品。



排序模型需要改进，**不能用全体用户模型**，否则会有严重的偏差

+ **大模型+小模型**，用全体用户行为训练大模型，大模型预估p拟合用户行为y，用特殊用户行为训练小模型，小模型的结果p拟合大模型的残差y-p。对特殊用户，**结合大模型和小模型的预估p+q**。
+ 融合多个experts，类似MMoE
+ 大模型预估后**小模型做校准**，特殊用户在使用全体数据训练的大模型推理后，在小模型上输入**大模型的结果和用户特征**，拟合真实数据。



如果对每类用户群体**单独使用一个排序模型**，可以**提升短期指标，但是维护代价大，长期有害**。



## 用户的交互行为

用户的**交互行为**可以带来很大的价值。



关注，**如果一个用户关注的作者越多，那么平台对其的吸引量就越大**。用户的留存率(r)和他关注的作者数量(f)呈正相关，如果用户的f较小，那么平台要促进该用户关注更多的作者。

<img src="./assets/image-20251019164808777.png" alt="image-20251019164808777" style="zoom:50%;" />



+ 在**融分公式**中添加一项**w(f)\*p**，其中p是模型预估出用户对候选物品的关注率，**w(f)是一个关于f递减的函数**，该指标可以促进用户关注作者。
+ 构建关注**内容池和召回通道**，内容池中物品的关注率高，**只对f较小的用户**使用该内容池。



被关注数(粉丝数)可以提升作者发布积极性，在融分公式中添加**w(fa)\*p**，其中p是预估关注率，**w(fa)是一个关于物品发布者粉丝数递减的函数**。



此外，挖掘**隐式关注关系**(经常看但未关注)也可以提升指标。



转发， **可以给平台带来站外流量**，同样在**融分公式中添加预估转发率乘权重的分数**，促进用户转发，但是转发率的权重要根据用户是不是**KOL**（外站大V，根据用户以往转发带来的流量判断），**如果是，则权重设置较高，反之接近于0**。



评论，融分公式中添加**评论率*与已有评论数量负相关的函数**。 







